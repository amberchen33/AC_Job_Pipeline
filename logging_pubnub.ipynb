{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygogo\n",
      "  Downloading pygogo-0.10.0-py2.py3-none-any.whl\n",
      "Collecting future~=0.15.2 (from pygogo)\n",
      "  Downloading future-0.15.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 257kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/echo030333/Library/Caches/pip/wheels/11/c5/d2/ad287de27d0f0d646f119dcffb921f4e63df128f28ab0a1bda\n",
      "Successfully built future\n",
      "Installing collected packages: future, pygogo\n",
      "Successfully installed future-0.15.2 pygogo-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pygogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import urllib\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "import gensim\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "import ujson\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "import logging\n",
    "import pygogo as gogo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pubnub\n",
      "  Downloading pubnub-4.0.13.tar.gz\n",
      "Collecting pycryptodomex>=3.3 (from pubnub)\n",
      "  Downloading pycryptodomex-3.4.6.tar.gz (6.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.5MB 82kB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.4 in ./anaconda/lib/python3.5/site-packages (from pubnub)\n",
      "Requirement already satisfied: six>=1.10 in ./anaconda/lib/python3.5/site-packages (from pubnub)\n",
      "Building wheels for collected packages: pubnub, pycryptodomex\n",
      "  Running setup.py bdist_wheel for pubnub ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/echo030333/Library/Caches/pip/wheels/fb/87/79/5af4a1a680d143563dec82391b37eb23d5ef5d8695d4cd4520\n",
      "  Running setup.py bdist_wheel for pycryptodomex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/echo030333/Library/Caches/pip/wheels/76/28/aa/d08cf510d56c3f0be340ef6a5453e970ce14ca82c1c2a899d9\n",
      "Successfully built pubnub pycryptodomex\n",
      "Installing collected packages: pycryptodomex, pubnub\n",
      "Successfully installed pubnub-4.0.13 pycryptodomex-3.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pubnub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pubnub.callbacks import SubscribeCallback\n",
    "from pubnub.enums import PNStatusCategory\n",
    "from pubnub.pnconfiguration import PNConfiguration\n",
    "from pubnub.pubnub import PubNub\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProcessScrape():\n",
    "\n",
    "   ##### Create logger #####\n",
    "   log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "   formatter = logging.Formatter(log_format)\n",
    "\n",
    "   logger = gogo.Gogo(\n",
    "       'status.fmt',\n",
    "       low_hdlr=gogo.handlers.file_hdlr('ProcessScrape.log'),\n",
    "       low_formatter=formatter,\n",
    "       high_level='info',\n",
    "       high_formatter=formatter).logger\n",
    "\n",
    "   def log_message(message):\n",
    "       logger.info(message)\n",
    "      \n",
    "   ### pubnub ###    \n",
    "       pnconfig = PNConfiguration()\n",
    "       pnconfig.subscribe_key = \"sub-c-417d8a10-76d3-11e7-81e6-0619f8945a4f\"\n",
    "       pnconfig.publish_key = \"pub-c-2882d102-7c5c-43e5-b6e1-096b075419fe\"\n",
    "       pnconfig.ssl = False\n",
    " \n",
    "       pubnub = PubNub(pnconfig)\n",
    "    \n",
    "       class MySubscribeCallback(SubscribeCallback):\n",
    "           def status(self, pubnub, status):\n",
    "               pass\n",
    "        # The status object returned is always related to subscribe but could contain\n",
    "        # information about subscribe, heartbeat, or errors\n",
    "        # use the operationType to switch on different options\n",
    "               if status.operation == PNOperationType.PNSubscribeOperation \\\n",
    "                    or status.operation == PNOperationType.PNUnsubscribeOperation:\n",
    "                    if status.category == PNStatusCategory.PNConnectedCategory:\n",
    "                        pass\n",
    "                # This is expected for a subscribe, this means there is no error or issue whatsoever\n",
    "                    elif status.category == PNStatusCategory.PNReconnectedCategory:\n",
    "                        pass\n",
    "                # This usually occurs if subscribe temporarily fails but reconnects. This means\n",
    "                # there was an error but there is no longer any issue\n",
    "                    elif status.category == PNStatusCategory.PNDisconnectedCategory:\n",
    "                        pass\n",
    "                # This is the expected category for an unsubscribe. This means there\n",
    "                # was no error in unsubscribing from everything\n",
    "                    elif status.category == PNStatusCategory.PNUnexpectedDisconnectCategory:\n",
    "                        pass\n",
    "                # This is usually an issue with the internet connection, this is an error, handle\n",
    "                # appropriately retry will be called automatically\n",
    "                    elif status.category == PNStatusCategory.PNAccessDeniedCategory:\n",
    "                        pass\n",
    "                # This means that PAM does allow this client to subscribe to this\n",
    "                # channel and channel group configuration. This is another explicit error\n",
    "                    else:\n",
    "                        pass\n",
    "                # This is usually an issue with the internet connection, this is an error, handle appropriately\n",
    "                # retry will be called automatically\n",
    "               elif status.operation == PNOperationType.PNSubscribeOperation:\n",
    "            # Heartbeat operations can in fact have errors, so it is important to check first for an error.\n",
    "            # For more information on how to configure heartbeat notifications through the status\n",
    "            # PNObjectEventListener callback, consult <link to the PNCONFIGURATION heartbeart config>\n",
    "                 if status.is_error():\n",
    "                    pass\n",
    "                # There was an error with the heartbeat operation, handle here\n",
    "                 else:\n",
    "                    pass\n",
    "                # Heartbeat operation was successful\n",
    "               else:\n",
    "                pass\n",
    "            # Encountered unknown status type\n",
    " \n",
    "           def presence(self, pubnub, presence):\n",
    "               pass  # handle incoming presence data\n",
    " \n",
    "           def message(self, pubnub, message):\n",
    "               pass  # handle incoming messages\n",
    " \n",
    " \n",
    "\n",
    "           def publish_callback(result, status):\n",
    "               pass\n",
    "    \n",
    "    # Handle PNPublishResult and PNStatus\n",
    "       pubnub.subscribe().channels('test').execute()   \n",
    "       pubnub.publish().channel('test').message(['logging', 'client', message() ]).async(publish_callback)\n",
    "\n",
    "\n",
    "   #####  Getting and Processing Data #####\n",
    "   log_message(\"Getting and Processing Data\")\n",
    "   _,_,files = next(os.walk(\"./JSONs/\"))\n",
    "   log_message(\"Number of files: \"+str(len(files)))\n",
    "\n",
    "   data = []\n",
    "   for filename in files:    \n",
    "       with open('./JSONs/'+filename, encoding=\"utf8\") as json_file:\n",
    "           json_data = json.load(json_file, object_pairs_hook=OrderedDict)\n",
    "           for listing in json_data:\n",
    "               data.append(listing)\n",
    "\n",
    "   for filename in files:\n",
    "       os.remove('./JSONs/'+filename)\n",
    "\n",
    "   #strips all non-ascii characters\n",
    "   #this is for all remaining non-significant ones\n",
    "   def make_ascii(text):\n",
    "       return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "   #strips out all html markup and insert ascii replacements\n",
    "   def cleanhtml(raw_html):\n",
    "       #replace for \\n\n",
    "       raw_html = raw_html.replace(\"<br>\",\"\\n\")\n",
    "       raw_html = raw_html.replace(\"</li>\",\"\\n\")\n",
    "       raw_html = re.sub(\"</h*>\",\"\\n\", raw_html)\n",
    "       raw_html = re.sub(\"</h3>\",\"\\n\", raw_html)\n",
    "       #html conversions\n",
    "       raw_html = raw_html.replace(\"&nbsp;\",\" \")\n",
    "       raw_html = raw_html.replace(\"   \",\" \")\n",
    "       raw_html = raw_html.replace(\"&#x27;\",\"'\")\n",
    "       raw_html = raw_html.replace(\"â\",\"'\")    \n",
    "       raw_html = raw_html.replace(\"â\",\"-\")\n",
    "       raw_html = raw_html.replace(\"â\",\"-\")\n",
    "       raw_html = raw_html.replace(\"â¢\",\"-\")\n",
    "       raw_html = raw_html.replace(\"â\",\"-\")\n",
    "       raw_html = raw_html.replace(\"&lt;\",\"<\")\n",
    "       raw_html = raw_html.replace(\"&gt;\",\">\")\n",
    "       raw_html = raw_html.replace(\"â¦\",\"...\")\n",
    "       raw_html = raw_html.replace(\"â¦\",\"...\")\n",
    "       raw_html = raw_html.replace(\"&amp\",\"and\")\n",
    "       raw_html = raw_html.replace(\"and;\",\"and\")\n",
    "       Raw_html = raw_html.replace(\"&\",\"and\")\n",
    "       raw_html = raw_html.replace(\"Ã©\",\"e\")\n",
    "       raw_html = raw_html.replace(\"â¬\",\"€\")\n",
    "       raw_html = raw_html.replace(\"â\",\"\\\"\") \n",
    "       raw_html = raw_html.replace(\"â\",\"\\\"\")       \n",
    "       raw_html = raw_html.replace(\"Â\",\"\")\n",
    "       raw_html = raw_html.replace(\"__\",\"\")\n",
    "       raw_html = raw_html.replace(\"_\",\"\")\n",
    "       raw_html = raw_html.replace(\"ï¿½\",\"\")\n",
    "       raw_html = re.sub(\"##+\",\"\",raw_html)\n",
    "       #remove noise markups\n",
    "       cleantext = raw_html\n",
    "       cleantext = re.sub('<.*?>','',cleantext)\n",
    "       cleantext = re.sub(' +',' ',cleantext)\n",
    "       cleantext = re.sub('(\\n)(\\n)(\\n)+','\\n\\n',cleantext)\n",
    "       return make_ascii(cleantext)\n",
    "\n",
    "   #iterate through listing descriptions and clean each one\n",
    "   for job_number, job in enumerate(data):   \n",
    "       job[\"description\"] = cleanhtml(str(job[\"description\"]))\n",
    "       job[\"label\"] = 0\n",
    "\n",
    "   raw_data = data\n",
    "\n",
    "   #####  OpenCalais #####\n",
    "   log_message(\"Using OpenCalais\")\n",
    "   # this is slow\n",
    "   # set API key and url.\n",
    "   access_token = 'AiYCbxhaNTAxGKBe5cl6KFWCM03GEEy3'\n",
    "   calais_url = 'https://api.thomsonreuters.com/permid/calais'\n",
    "\n",
    "   for i, row in enumerate(raw_data):\n",
    "       input_data = row['description']\n",
    "       \n",
    "       # set headers for Calais.\n",
    "       headers = {'X-AG-Access-Token' : access_token, 'Content-Type' : 'text/raw', 'outputformat' : 'application/json'}    \n",
    "              \n",
    "       if(i%20==0):\n",
    "          log_message(i+1,'/',len(raw_data))\n",
    "       resp_json = \"nothing here\"\n",
    "       for i in range(10):\n",
    "           try:\n",
    "               response = requests.post(calais_url, data=input_data,headers=headers)\n",
    "               resp_json = response.json()\n",
    "               break\n",
    "           except:\n",
    "               continue\n",
    "       if(resp_json==\"nothing here\"):\n",
    "           continue\n",
    "                   \n",
    "       new_tags = []\n",
    "       keys = resp_json.keys()\n",
    "       for key in keys:\n",
    "           field = resp_json[key]\n",
    "           try:\n",
    "               typeGroup = field['_typeGroup']\n",
    "               if(typeGroup==\"socialTag\"):\n",
    "                   new_tags.append(field['name'])\n",
    "           except:\n",
    "               continue\n",
    "\n",
    "       new_tags = list(set(new_tags))\n",
    "       tags = row['tags']\n",
    "       for tag in new_tags:\n",
    "           tags.append(tag)\n",
    "       row['tags'] = tags\n",
    "\n",
    "   ##### Formatting and Processing Data #####\n",
    "   log_message(\"Formatting and Processing Data\")\n",
    "\n",
    "   #split features based on symbol\n",
    "   def resplit(arr,symbol):\n",
    "       retarr = []\n",
    "       for value in arr:\n",
    "           vals = value.split(symbol)\n",
    "           for val in vals:\n",
    "               retarr.append(val)\n",
    "       return retarr\n",
    "\n",
    "   #load in full data\n",
    "   full_data = {}\n",
    "\n",
    "   for i, row in enumerate(raw_data):\n",
    "       myID = row['id']\n",
    "       tags = row['tags']\n",
    "       if(tags==None):\n",
    "           tags=\"[]\"\n",
    "       position = row['position']\n",
    "       features = [] \n",
    "               \n",
    "       #split tags and position based on any symbols\n",
    "       splts = [\" \",\"-\",\"/\"]\n",
    "       for splt in splts:\n",
    "           tags = resplit(tags,splt)\n",
    "       for tag in tags:\n",
    "           features.append(tag)\n",
    "                \n",
    "       splts = [\"-\",\"/\"]\n",
    "       position = position.split(\" \")\n",
    "       for splt in splts:\n",
    "           position = resplit(position,splt)\n",
    "       for pos in position:\n",
    "           features.append(pos)\n",
    "    \n",
    "       full_data[myID] = features \n",
    "            \n",
    "   for myID in full_data:\n",
    "       features = full_data[myID]\n",
    "       for num_feat, feature in enumerate(features): \n",
    "           #strip any excess symbols\n",
    "           strip_items = [\"(\",\")\",\"-\",\"+\",\"/\",\"&\"]\n",
    "           for strip_item in strip_items:\n",
    "               feature = feature.strip(strip_item)\n",
    "               features[num_feat] = feature.strip(strip_item)                        \n",
    "       while('' in features):\n",
    "           features.remove('')\n",
    "\n",
    "   #reformatted data\n",
    "   label_data = []\n",
    "   for myID in full_data:\n",
    "       params = [myID, full_data[myID]]\n",
    "       label_data.append(params)\n",
    "\n",
    "   ##### Word2Vec and Preprocessing #####\n",
    "   log_message(\"Word2Vec and Preprocessing\")\n",
    "\n",
    "   #loads in pretrained word2vec model\n",
    "   #to get vector, run as dictionary\n",
    "   model = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "   X = []\n",
    "   for row in label_data:\n",
    "       words = row[1]\n",
    "       words_vec = []\n",
    "       for word in words:\n",
    "           try:\n",
    "               words_vec.append(model[word])\n",
    "           except:\n",
    "               continue\n",
    "       X.append(words_vec)\n",
    "\n",
    "   ##### Classifier #####\n",
    "   log_message(\"Classifier\")\n",
    "\n",
    "   #loads saved model\n",
    "   acc_name=\"36.32\"\n",
    "   clf = joblib.load('./models/'+'svm_'+acc_name+'.pkl')\n",
    "\n",
    "   y = np.zeros((len(X)))\n",
    "   for user_index, user in enumerate(X):\n",
    "       cats = np.zeros((7))\n",
    "       try:\n",
    "           user_pred = clf.predict(user)\n",
    "       except: #result is misc\n",
    "           cats[6]+=1\n",
    "           continue\n",
    "       for pred in user_pred:\n",
    "           for i in range(7):\n",
    "               if( abs(pred-(i+1)) <.1):\n",
    "                   cats[i]+=1       \n",
    "       for i in range(7):\n",
    "           cats[i] += i/10\n",
    "       y[user_index] = np.argmax(cats)+1\n",
    "\n",
    "   hashed_labels = {}\n",
    "   for i, row in enumerate(label_data):\n",
    "       hashed_labels[row[0]] = int(y[i])\n",
    "\n",
    "   for listing in raw_data:\n",
    "       listing['label'] = hashed_labels[listing['id']]\n",
    "\n",
    "   ##### Data Changes #####\n",
    "   log_message(\"Data Changes\")\n",
    "\n",
    "   for listing in raw_data:\n",
    "       checks = ['company','description','epoch','position','slug']\n",
    "       for check in checks:\n",
    "           if(listing[check]==None):\n",
    "               listing[check] = \"No \"+check+\" provided\"\n",
    "\n",
    "   #add timepast\n",
    "   today = datetime.datetime.today()\n",
    "   for listing in raw_data:\n",
    "       date = listing['date']\n",
    "       if(date==None):\n",
    "           listing['timepast'] = 100\n",
    "           continue\n",
    "       date = datetime.datetime.strptime(date[0:10], '%Y-%m-%d')\n",
    "       date = date.replace(tzinfo=None)\n",
    "       difference = today - date\n",
    "       listing['timepast'] = difference.days\n",
    "\n",
    "   #format dates\n",
    "   for listing in raw_data:\n",
    "       date = listing['date']\n",
    "       if(date==None):\n",
    "           listing['date'] = \"No date provided\"\n",
    "           continue\n",
    "       date = datetime.datetime.strptime(date[0:10], '%Y-%m-%d')\n",
    "       disp_date = date.strftime('%b %d, %Y')\n",
    "       listing['date'] = disp_date\n",
    "\n",
    "   #format tags\n",
    "   for listing in raw_data:\n",
    "       tags = listing['tags']\n",
    "       tags_disp = \"No tags\"\n",
    "       if(len(tags)>0):\n",
    "           tags_disp = \"\"\n",
    "           for tag in tags:\n",
    "               tag = tag.title()\n",
    "               tags_disp += tag+\", \"\n",
    "           tags_disp = tags_disp[0:len(tags_disp)-2]\n",
    "       tags_arr = tags_disp.split(\", \")\n",
    "       listing['tags'] = tags_arr\n",
    "       listing['tags_display'] = tags_disp\n",
    "\n",
    "   #Uses first paragraph as description\n",
    "   for listing in raw_data:\n",
    "       description = listing['description']\n",
    "       if(description==None):\n",
    "           description = \"No description provided.\"\n",
    "       else:\n",
    "           description = description.split(\"\\n\")[0]\n",
    "       listing['description'] = description\n",
    "\n",
    "   #Gives dummy logos to those without\n",
    "   for listing in raw_data:\n",
    "       logo = listing['logo']\n",
    "       if(logo==None or len(logo)<3):\n",
    "           logo = \"https://dummyimage.com/170/000/ffffff&text=\"+(listing['company'].replace(' ','%20'))\n",
    "       listing['logo'] = logo\n",
    "\n",
    "   # Converts labels to categories\n",
    "   ## Labels: 1=Interface Tech Arch(ITA), 2=Software Tech Arch(STA), 3=Database Tech Arch(DTA), 4=System Tech Arch(SysTA),\n",
    "   # 5=Bus Anal (BA), 6=Proj Man (PM), 7=N/A\n",
    "   label_to_name = {\n",
    "       \"0\": \"Uncategorized\",\n",
    "       \"1\": \"Interface Technical Architect\",\n",
    "       \"2\": \"Software Technical Architect\",\n",
    "       \"3\": \"Database Technical Architect\",\n",
    "       \"4\": \"System Technical Architect\",\n",
    "       \"5\": \"Business Analyst\",\n",
    "       \"6\": \"Project Manager\",\n",
    "       \"7\": \"Miscellaneous\"\n",
    "   }\n",
    "   for listing in raw_data:\n",
    "       listing['label'] = label_to_name[str(listing['label'])]\n",
    "\n",
    "   for listing in raw_data:\n",
    "       listing['title'] = \"title\"\n",
    "\n",
    "   for listing in raw_data:    \n",
    "       if(listing['timepast']==0):\n",
    "           listing['new'] = \" [New!]\"\n",
    "       else:\n",
    "           listing['new'] = \"\"\n",
    "\n",
    "   ##### JSON Generation #####\n",
    "   log_message(\"JSON Generation\")\n",
    "   file_name = \"mindy_els\"\n",
    "   with open(file_name+\".json\", 'w', encoding=\"utf8\") as outfile:\n",
    "       json.dump(raw_data, outfile)\n",
    "\n",
    "   ##### ElasticSearch #####\n",
    "   log_message(\"ElasticSearch\")\n",
    "   es = Elasticsearch()\n",
    "   with open(file_name+\".json\", 'r', encoding=\"utf8\") as outfile:\n",
    "       data = json.load(outfile)    \n",
    "       helpers.bulk(es, data, index='my_index', doc_type='my_type')\n",
    "\n",
    "   #gets duplicates\n",
    "   def fetch_duplicates():\n",
    "       uri = \"http://mindy-elastic:9200/my_index/_search\"\n",
    "       payload = {\"size\": 0,\n",
    "                   \"aggs\": {\n",
    "                       \"duplicateCount\": {\"terms\":\n",
    "                               {\"field\": \"id\", \"min_doc_count\": 2, \"size\": 10000},\n",
    "                                   \"aggs\": {\n",
    "                                       \"duplicateDocuments\":\n",
    "                                       {\"top_hits\": {}}\n",
    "                                   }\n",
    "                               }\n",
    "                       }\n",
    "                  }\n",
    "       json = ujson.dumps(payload)\n",
    "       resp = requests.post(uri, data=json)\n",
    "       ret = ujson.loads(resp.text)\n",
    "       return ret\n",
    "\n",
    "   # generates commands to delete\n",
    "   def delete_query(buf, index, doc_type, i):\n",
    "       buf.write('{\"delete\":{\"_index\":\"'+index+'\",\"_type\":\"'+doc_type+'\",\"_id\":\"'+i+'\"}}\\n')\n",
    "\n",
    "   # bulk deletes and counts\n",
    "   def bulk_remove(buf):\n",
    "       try:\n",
    "           uri = \"http://mindy-elastic:9200/_bulk?refresh=wait_for\"\n",
    "           resp = requests.post(uri, data=buf.getvalue())\n",
    "           if resp.status_code == 200:\n",
    "               r = ujson.loads(resp.text)\n",
    "               if r['errors']:\n",
    "                   log_message(r)\n",
    "               cnt = 0\n",
    "               for item in r['items']:\n",
    "                   if ('found' in item['delete']) and item['delete']['found']:\n",
    "                       cnt += 1\n",
    "                   else:\n",
    "                       log_message(item)\n",
    "               return cnt\n",
    "           elif(resp.text == '''{\"error\":{\"root_cause\":[{\"type\":\"parse_exception\",\"reason\":\"request body is required\"}],\"type\":\"parse_exception\",\"reason\":\"request body is required\"},\"status\":400}'''):\n",
    "               log_message(\"No more to remove\")\n",
    "           else:\n",
    "               log_message(\"failed to fetch duplicates: #{0}\".format(resp.text))\n",
    "       except requests.exceptions.ConnectionError as e:\n",
    "           logger.error(\"ERROR: connection failed, check --host argument and port. Is ES running on http://mindy-elastic:9200?\")\n",
    "           logger.error(e)\n",
    "\n",
    "   # keeps calling the remove process until all are removed\n",
    "   # this is necessary because of how elastic shards data\n",
    "   # sometimes duplicates are not caught due to this and requires loop\n",
    "   duplicatesExist = 2 #check twice\n",
    "   timer = 0 #prevent infinite loop\n",
    "   while(duplicatesExist>0 and timer<10):\n",
    "       resp = fetch_duplicates()\n",
    "       docs = len(resp[\"aggregations\"][\"duplicateCount\"][\"buckets\"])    \n",
    "       if(docs==0):\n",
    "           duplicatesExist-=1\n",
    "       removed = remove_duplicates(resp, \"my_index\", \"my_type\")\n",
    "       log_message(docs,\" \",removed)\n",
    "       timer+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
